---
title: "Guatemala Parcels EDA"
author: "Ozzy Houck"
date: today
format: 
    typst:
        toc: true
        section-numbering: 1.1.a
        columns: 2
execute:
  eval: false 
  echo: false
---

### Set up
```{python}
import os
import ee
import geemap
import socket
import pandas as pd
import numpy as np
import geopandas as gpd
from shapely.geometry import box 
from shapely.geometry import mapping
import re # regular expressions
import folium
from folium import GeoJson
from folium.plugins import MarkerCluster
import rasterio
from rasterio.merge import merge
from rasterio.mask import mask
from rasterio.features import geometry_mask
from rasterio.warp import calculate_default_transform, reproject, Resampling
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import json
import warnings


# Initialize Earth Engine
ee.Initialize()

def setup_directories():
    # check if we are on the server or local
    nodename = socket.gethostname()
    if nodename == "oMac.local": # local laptop
        root = os.path.expanduser("~/OneDrive - The University of Chicago/guatamala_ag/data")
    else:
        raise Exception("Unknown environment, Please specify the root directory")

    dirs = {
        'root': root,
        'raw': os.path.join(root, "raw"),
        'processed': os.path.join(root, "processed"),
        'fig': os.path.join(root, "../figures")
    }

    for path in dirs.values():
        os.makedirs(path, exist_ok=True)

    return dirs

dir = setup_directories()

```

### Clean Coordinates

```{python}
def add_decimal_if_missing(coord):
    if isinstance(coord, str) and coord.replace('-', '').isdigit():
        if coord.startswith('-'):
            return f"-{coord[1:3]}.{coord[3:]}"
        else:
            return f"{coord[:2]}.{coord[2:]}"
    return coord

def is_valid_guatemala_coordinate(lat, lon):
    # Approximate bounding box for Guatemala
    return 13.1 <= lat <= 18.2 and -93 <= lon <= -88.0

def fix_known_coordinate_issues(df):
    """
    Fix known coordinate issues in the dataframe using approximate floating-point comparisons.
    """
    known_fixes = {
        ('longitude_4', 90.366662): -90.366662,
        ('latitude_1', 5.267439): 15.267439,
        ('longitude_1', -16.352620): -90.1635620,
    }
    
    for (col, incorrect_value), correct_value in known_fixes.items():
        # Use numpy's isclose for approximate floating-point comparison
        mask = np.isclose(df[col], incorrect_value, rtol=1e-5, atol=1e-8)
        if mask.any():
            df.loc[mask, col] = correct_value
            print(f"Fixed {mask.sum()} occurrences of approximately {incorrect_value} to {correct_value} in {col}")
    
    return df

def split_coordinates(coord_str):
    # Existing manual fixes
    manual_fixes = {
        "16,3870407, -89,7345351": "16.3870407, -89.7345351",
        "14.177150.3,-90.3989608": "14.1771503, -90.3989608",
        "16,3869101, -89,7344694": "16.3869101, -89.7344694",
        "14.141996-90-147208": "14.141996, -90.147208",
        "16,3871767, -89,7348127": "16.3871767, -89.7348127",
        "16,3869863, -89,7349780": "16.3869863, -89.7349780"
    }
    
    if coord_str in manual_fixes:
        coord_str = manual_fixes[coord_str]
    
    # Remove any quotation marks and leading/trailing whitespace
    cleaned = coord_str.strip().strip('"')
    
    # Try to match various patterns
    patterns = [
        r'^([-]?\d+\.?\d*)[,\s]+([-]?\d+\.?\d*)$',  # Comma or space separated
        r'^([-]?\d+\.?\d*)\.([-]?\d+\.?\d*)$',      # Period separated
        r'^(\d+\.?\d*)(-\d+\.?\d*)$'                # No separator with negative longitude
    ]
    
    for pattern in patterns:
        # if we get passed the first pattern, remove all whitespace
        cleaned = re.sub(r'\s', '', cleaned)
        match = re.match(pattern, cleaned)
        if match:
            lat, lon = match.group(1), match.group(2)
            lat = add_decimal_if_missing(lat)
            lon = add_decimal_if_missing(lon)
            return pd.Series({'latitude': lat, 'longitude': lon})
    
    # If we couldn't split it, return empty strings
    print(f"Could not split coordinates: {coord_str}")
    return pd.Series({'latitude': '', 'longitude': ''})

df = pd.read_excel(os.path.join(dir['raw'], "Datos de Impacto Productores 2023.xlsx"), 
    sheet_name= 0, skiprows=4)

vars_to_keep = ["id_phone", "id_coordinates_1", "id_coordinates_2", 
                "id_coordinates_3", "id_coordinates_4"]
df = df[vars_to_keep]
# drop rows with missing id_coordinates_1
df = df.dropna(subset=["id_coordinates_1"])

# Process coordinates
for i in range(1, 5):
    col_name = f'id_coordinates_{i}'
    new_cols = df[col_name].apply(split_coordinates)
    df[f'latitude_{i}'] = pd.to_numeric(new_cols['latitude'], errors='coerce')
    df[f'longitude_{i}'] = pd.to_numeric(new_cols['longitude'], errors='coerce')
    
    # Check if coordinates are within Guatemala's range
    df[f'valid_coordinate_{i}'] = df.apply(
        lambda row: is_valid_guatemala_coordinate(row[f'latitude_{i}'], row[f'longitude_{i}']), 
        axis=1
    )

# Fix known coordinate issues
df = fix_known_coordinate_issues(df)

# Recheck validity after fixes
for i in range(1, 5):
    df[f'valid_coordinate_{i}'] = df.apply(
        lambda row: is_valid_guatemala_coordinate(row[f'latitude_{i}'], row[f'longitude_{i}']), 
        axis=1
    )

# Print summary of remaining invalid coordinates
for i in range(1, 5):
    invalid_coords = df[~df[f'valid_coordinate_{i}']]
    if not invalid_coords.empty:
        print(f"\nRemaining invalid coordinates for id_coordinates_{i}:")
        print(invalid_coords[[f'latitude_{i}', f'longitude_{i}']])

# Check if all coordinates are valid
all_valid = df.apply(lambda row: all(row[f'valid_coordinate_{i}'] for i in range(1, 5)), axis=1)
print(f"\nTotal rows with all valid coordinates: {all_valid.sum()} out of {len(df)}")

# merge to yield in another sheet
df_yield = pd.read_excel(os.path.join(dir['raw'], "Datos de Impacto Productores 2023.xlsx"), 
    sheet_name= 1, skiprows=2)

df_yield = df_yield[["id_phone", "harv_product_qqmz"]]
# remove rows with missing values
df_yield = df_yield.dropna(subset=["harv_product_qqmz"])

# merge the two dataframes
df = df.merge(df_yield, on="id_phone", how="inner")
print(f"\nMerged data shape: {df.shape}")

# Save the processed data
df.to_csv(os.path.join(dir['processed'], "farm_coordinates.csv"), index=False)
print("\nData processing complete. Results saved to 'farm_coordinates.csv'.")
```

### Convert to GeoDataFrame and Calculate Area
```{python}
# create lat and lon min and max columns
df['lat_min'] = df[['latitude_1', 'latitude_2', 'latitude_3', 'latitude_4']].min(axis=1)
df['lat_max'] = df[['latitude_1', 'latitude_2', 'latitude_3', 'latitude_4']].max(axis=1)
df['lon_min'] = df[['longitude_1', 'longitude_2', 'longitude_3', 'longitude_4']].min(axis=1)
df['lon_max'] = df[['longitude_1', 'longitude_2', 'longitude_3', 'longitude_4']].max(axis=1)

# Function to create a polygon from min/max coordinates
# jury is out on which is better
def create_polygon(row):
    return box(row['lon_min'], row['lat_min'], row['lon_max'], row['lat_max'])

# Create the geometry column
df['geometry'] = df.apply(create_polygon, axis=1)

crs = "EPSG:5459" # crs for guatemala
gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=crs)

# Function to calculate area in square meters
def calculate_area(geometry, lat):
    # Define a local projection centered on the polygon
    local_azimuthal_projection = f"+proj=aeqd +lat_0={lat}\
        +lon_0={geometry.centroid.x} +x_0=0 +y_0=0"
    
    # Create a GeoSeries with the input geometry and set its CRS
    geoseries = gpd.GeoSeries([geometry], crs="EPSG:4326")
    
    # Project the GeoSeries to the local azimuthal equidistant projection
    projected_geoseries = geoseries.to_crs(local_azimuthal_projection)
    
    # Get the projected geometry and calculate its area
    projected_geometry = projected_geoseries.iloc[0]

    area = projected_geometry.area

    # check if area is nan
    if pd.isna(area):
        print(f"Area is nan for {geometry}")
        # print the lat and lon
        print(f"Lat: {lat}, Lon: {geometry.centroid.x}")
        return

    return area 

# create an id column
gdf['id'] = range(len(gdf))

# Calculate area for each polygon
gdf['area_sqm'] = gdf.apply(lambda row: calculate_area(row['geometry'], 
                            row['geometry'].centroid.y), axis=1)

# count number of rows with area over 1 million sqm
print(f"Number of rows with area over 500k sqm: {len(gdf[gdf['area_sqm'] > 500_000])}")

# drop rows with area over 0.5 million sqm (5k by 5k meters)
gdf = gdf[gdf['area_sqm'] < 500_000]

# Display info about the GeoDataFrame
print(f"\nGeoDataFrame shape: {gdf.shape}")
print(f"GeoDataFrame CRS: {gdf.crs}")

# print summary statistics for area and round to 2 decimal places
print(gdf['area_sqm'].describe().round(2))

# keep only the columns we need
columns_to_keep = ['id', 'area_sqm', 'geometry', "harv_product_qqmz"]
gdf = gdf[columns_to_keep]

# save csv with id and harv_product_qqmz to use as a crosswalk
crosswalk = gdf[['id', 'harv_product_qqmz']]
crosswalk.to_csv(os.path.join(dir['processed'], "id_harvest_crosswalk.csv"), index=False)

gdf_polygons = ee.Geometry.MultiPolygon(list(gdf.geometry.apply(lambda x: list(mapping(x)['coordinates'][0]))))

```

### Create Interactive Map showing sentenial 2 imagery and parcel boundaries using GEE

```{python}
import geemap
import ee
import pandas as pd
import os

# Initialize the Earth Engine library
ee.Initialize()

# Create an interactive map
Map = geemap.Map()

# Load the Sentinel-2 image collection
s2_collection = (ee.ImageCollection('COPERNICUS/S2_SR')
                 .filterDate('2023-01-03', '2023-06-30')
                 .filterBounds(ee.FeatureCollection(gdf.__geo_interface__))
                 .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))
                 .select(['B2', 'B3', 'B4', 'B8']) 
)

# Create a median composite
s2_median = s2_collection.median()

# Clip to parcel boundaries
clipped = s2_median.clip(ee.FeatureCollection(gdf.__geo_interface__))

# Add NDVI computation
def add_ndvi(image):
    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')
    return image.addBands(ndvi)

s2_median = add_ndvi(s2_median)


#==============================================================================
# Sidetrack to make a map
#==============================================================================
# Visualization parameters
ndvi_vis_params = {
    'min': 0,
    'max': 1,
    'palette': ['blue', 'white', 'green']
}

rgb_vis_params = {
    'bands': ['B4', 'B3', 'B2'],  # RGB bands
    'min': 0,
    'max': 3000
}

# Add layers to the map
# Add a base map with streets and administrative regions
Map.add_basemap('HYBRID')
Map.addLayer(s2_median.select('NDVI'), ndvi_vis_params, 'NDVI')
Map.addLayer(s2_median, rgb_vis_params, 'Sentinel-2 RGB')

# Add translucent red boxes for parcels
parcel_style = {
    'color': 'red',
    'fillColor': 'red',
    'fillOpacity': 0.7
}
Map.add_gdf(gdf, layer_name='Farm Parcels', style=parcel_style)

# Add a LayerControl widget for toggling layers
Map.addLayerControl()

# Zoom in on the first parcel
centroid = gdf.iloc[0].geometry.centroid
Map.setCenter(centroid.x, centroid.y, 16)

# Save and display the map
output_file = os.path.join(dir['fig'], 'interactive_map_geemap.html')
Map.save(output_file)
print(f"Interactive map saved to {output_file}")

#==============================================================================
# 2. Export 64x64 patches around each farm’s centroid
#==============================================================================

# For Sentinel-2 at 10m resolution, a 64×64 patch is 640×640 meters.
# We'll create a 640m bounding box around the centroid of each parcel.
output_dir = os.path.join(dir["processed"], "farm_patch_sentinel2")
os.makedirs(output_dir, exist_ok=True)

for idx, row in gdf.iterrows():
    farm_id = row["id"]
    
    # Convert Shapely geometry to EE geometry
    farm_geom_ee = ee.Geometry(mapping(row["geometry"]))
    
    # Get centroid in Earth Engine
    centroid = farm_geom_ee.centroid()
    
    # Create a ~640m bounding box around the centroid:
    #    buffer(320) => circle of radius 320 m
    #    .bounds()   => convert that circle to a bounding rectangle
    patch_region = centroid.buffer(320).bounds()
    
    # Clip the median composite to that patch region
    patch_image = s2_median.clip(patch_region)
    
    # Build output path
    out_file = os.path.join(output_dir, f"{farm_id}.tif")
    
    # Export the patch as a GeoTIFF.  
    #   scale=10 → each pixel = 10m, so final image ~ 64×64 pixels
    #   region   → bounding geometry
    #   file_per_band=False → combine bands into one file (instead of multiple .tif files)
    geemap.ee_export_image(
        ee_object=patch_image.select('B2', 'B3', 'B4'),  #OH: will want to change to add NDVI, might require modifying pretrained model
        filename=out_file,
        scale=10,
        region=patch_region.getInfo()["coordinates"],
        crs="EPSG:4326",           # adjust if you want another projection
        file_per_band=False
    )
    
    print(f"Saved 64×64 NDVI patch for parcel {farm_id} → {out_file}")
```

# Pytorch model

# todo: implement this code from https://chatgpt.com/c/67742cbe-fb50-8007-bf19-b5527e012838



## Set up the PyTorch Dataset
```{python}
import os
import torch
import numpy as np
import pandas as pd
from torchgeo.datasets import RasterDataset


class Sentinel2FarmParcels(RasterDataset):
    """
    A TorchGeo-style dataset for multi-band Sentinel-2 .tif images stored locally.
    
    Expects:
        - Each farm has exactly one multi-band .tif (e.g. B2, B3, B4, B8 stacked).
        - A CSV mapping farm_id -> yield.
    """
    def __init__(self, root: str, csv_file: str, transforms=None) -> None:
        """
        Args:
            root      (str): Directory containing multi-band .tif files.
            csv_file  (str): Path to CSV with columns [farm_id, yield].
            transforms     : Optional transforms/augmentations to apply.
        """
        # Pass root and transforms to the parent RasterDataset (custom Raster data)
        super().__init__(paths=root, transforms=transforms)
        self.root = root
        self.df = pd.read_csv(csv_file)
        self.transforms = transforms


        # We'll store "samples" with { 'path': <path to .tif>, 'yield': <float> }
        self.samples = []
        for _, row in self.df.iterrows():
            farm_id = int(row["id"])
            yield_val = float(row["harv_product_qqmz"])
            tif_path = os.path.join(self.root, f"{farm_id}.tif")

            if not os.path.isfile(tif_path):
                raise FileNotFoundError(
                    f"Expected file {tif_path} for farm_id={farm_id}, but not found."
                )

            self.samples.append({
                "farm_id": farm_id,
                "yield": yield_val,
                "path": tif_path
            })

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, index: int):
        """
        Returns:
            image_tensor: (C, H, W) torch.FloatTensor
            label_tensor: (1,) torch.FloatTensor
        """
        sample = self.samples[index]
        tif_path = sample["path"]
        yield_val = sample["yield"]


       # load multiband raster using rasterio
        with rasterio.open(tif_path) as src:
            img_array = src.read()  # shape: (bands, height, width)
        # Convert to float32
        img_array = img_array.astype(np.float32)

        # 2) Wrap image and label in torch tensors
        image_tensor = torch.from_numpy(img_array)             # shape: (C, H, W)
        label_tensor = torch.tensor([yield_val], dtype=torch.float32)  # shape: (1,)

        # 3) Apply any optional transforms or augmentations
        #    (Might need a custom transform function if you use geospatial ops)
        if self.transforms:
            image_tensor, label_tensor = self.transforms(image_tensor, label_tensor)

        return image_tensor, label_tensor
```


```{python}
dataset = Sentinel2FarmParcels(
    root= os.path.join(dir['processed'], "farm_patch_sentinel2"),
    csv_file=os.path.join(dir['processed'], "id_harvest_crosswalk.csv")
)

print(f"Dataset length: {len(dataset)}")

image, label = dataset[40]
print("Image shape:", image.shape)  # e.g., (4, H, W) if you have 4 bands
print("Label:", label.item())       # yield float

from torch.utils.data import random_split, DataLoader

# split into train and validation sets
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_ds, val_ds = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=1)
val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=1)

# helper function to plot an image in the dataset
def plot_image(image, label):
    # Plot the RGB image
    # OH: note will have to modify if more than rgb bands are used
    # b2, b3, b4 is blue, green red so reorder to rgb
    rgb_indices = [2, 1, 0]
    rgb_image = image[rgb_indices, :, :].numpy()  # Convert to NumPy array and put colors in corect order
    rgb_image = np.moveaxis(rgb_image, 0, -1) # move rbg to last axis
    # normalize rbg values to 0-1
    rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min())
    plt.imshow(rgb_image)
    plt.title(f"Yield: {label.item()}")
    plt.axis("off")
    plt.show()

plot_image(image, label)

```

## Train a model using resnet50

# OH to do: Figure out how to initialize weights from other modes and deal with run time error

```{python}
import torch
import torch.nn as nn
import torch.optim as optim
from torchgeo.models import resnet18, ResNet18_Weights

# Example: BigEarthNet pretrained ResNet18
# model = resnet18(weights=ResNet18_Weights.SENTINEL2_RGB_MOCO)
model = resnet18(weights=None)
# ... Adapt first conv for # of channels if needed, adapt final layer for 1 output ...
# model = model.cuda()

# Want to predict yield and not class so change final layer to have 1 output
model.fc = nn.Linear(model.fc.in_features, 1)

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

epochs = 5

for epoch in range(epochs):
    # Training loop
    model.train()
    running_loss = 0.0
    for images, labels in train_loader:
        images = images.cuda()
        labels = labels.cuda()

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    train_loss = running_loss / len(train_loader)

    # Validation loop, etc.
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for images, labels in val_loader:
            images = images.cuda()
            labels = labels.cuda()

            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
    val_loss /= len(val_loader)

    print(f"Epoch [{epoch+1}/{epochs}] - "
          f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
```


# scrap yard
```{python}
# CODE FOR CREATING INTERACTIVE MAP AND EXTRACTING MEAN NDVI
Map = geemap.Map()

# Load the Sentinel-2 image collection
s2_collection = (ee.ImageCollection('COPERNICUS/S2_SR')
                 .filterDate('2023-01-01', '2023-06-30')
                 .filterBounds(ee.FeatureCollection(gdf.__geo_interface__))
                 .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))
                 .select(['B8', 'B5', 'B4', 'B3', 'B2']))

# Create a median composite
s2_median = s2_collection.median()

clipped = s2_median.clip(gdf_polygons)

def add_ndvi(image):
    # Note B5 is NIR  at 20m resolution wavelength = 705nm
    # B8 is NVIR at 10m resolution wavelength = 852nm 
    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')
    return image.addBands(ndvi)
# clipped = add_ndvi(clipped)
s2_median = add_ndvi(s2_median)

def zonal_stats(geometry, image):
    reducer = ee.Reducer.mean()
    stats = image.reduceRegion(
        reducer=reducer,
        geometry=geometry,
        scale=10, # sentinel 2 resolution is 10m
        maxPixels=1e8
    )
    return stats

results = []

# Calculate zonal stats for each parcel
for idx, row in gdf.iterrows():
    parcel_geom = ee.Geometry.Polygon(list(row['geometry'].exterior.coords))
    stats = zonal_stats(parcel_geom, s2_median)

    # mean valuse for hte bands and NDVI
    mean_values = stats.getInfo()

    # append to results
    results.append({
        'id': row['id'],
        'mean_B2': mean_values['B2'],
        'mean_B3': mean_values['B3'],
        'mean_B4': mean_values['B4'],
        'mean_B8': mean_values['B8'],
        'mean_NDVI': mean_values['NDVI']
    })

# Convert results to a pandas DataFrame
results_df = pd.DataFrame(results)

# merge back to the original geodataframe
gdf_sentinel = gdf.merge(results_df, on='id')

# save the geodataframe
gdf_sentinel.to_file(os.path.join(dir['processed'], "farm_parcels.geojson"), driver='GeoJSON')

# Add the Sentinel-2 layer
Map.addLayer(clipped, {'min': 0, 'max': 3000}, 'Sentinel-2 Median')

# Add the farm parcels
Map.add_gdf(gdf, layer_name = 'Farm Parcels', fill_colors=['red'])

# zoom in on the first parcel
centroid = gdf.iloc[0].geometry.centroid
Map.setCenter(centroid.x, centroid.y, 16)

# Display the map
Map.save(os.path.join(dir['fig'], 'interactive_map_geemap.html'))
print(f"Interactive map saved to {os.path.join(dir['fig'], 'interactive_map_geemap.html')}")
```

```{python}
# read in the geodataframe
gdf_sentinel = gpd.read_file(os.path.join(dir['processed'], "farm_parcels.geojson"))

# plot relationship between NDVI and yield
plt.figure(figsize=(10, 6))
plt.scatter(gdf_sentinel['mean_NDVI'], gdf_sentinel['harv_product_qqmz'])
plt.xlabel('Mean NDVI')
plt.ylabel('Harvested Product (QQMZ)')
plt.title('Relationship between NDVI and Harvested Product')
plt.savefig(os.path.join(dir['fig'], 'scatter_ndvi_vs_yield.png'))

# split the data
X = gdf_sentinel[['mean_NDVI']]
y = gdf_sentinel['harv_product_qqmz']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# fit the model
# run naive ols prediction of yield using NDVI
model = LinearRegression()
model.fit(X_train, y_train)

# make predictions
y_pred = model.predict(X_test)

# calculate the mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, color='blue', label='True values')
plt.scatter(X_test, y_pred, color='red', label='Predicted values')
plt.xlabel('Mean NDVI')
plt.ylabel('Harvested Product (QQMZ)')
plt.title('Predicted vs. True Harvested Product')
plt.legend()
plt.savefig(os.path.join(dir['fig'], 'naive_ols_yield_prediction.png'))
```